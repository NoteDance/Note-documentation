Use kernel to train neural network. First, you should write a neural network class that include method object and Instance object.
Method object:
Method object need to use Tensorflow 2.0 version and above to write.
1. fp(data) forward propagation function,it receive data,return output.
2. loss(output,labels) loss function,it receive output and labels output loss.
You can also do.
1. fp(data,labels) forward propagation function,it receive data and labels,return output and loss.
2. opt(gradient).

Instance object:
1.If you don't want to write optimizer by yourself,assign opt as optimizer of tensorflow.
2.Instance include parameter and hyper-parameter,where parameter must be named param, instance can include anything you need.


----------------------------------------------------------------------------------------
If you accomplish your neural network,you can use kernel to train.
example:
import Note.create.DL.kernel as k   #import kernel
import tensorflow as tf                   #import platform
import cnn as c                               #import neural network
mnist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train,x_test =x_train/255.0,x_test/255.0
y_train=tf.one_hot(y_train,10)
cnn=c.cnn()                                #create neural network object
kernel=k.kernel(cnn)                 #start kernel
kernel.platform=tf                           #use platform
kernel.data(x_train,y_train)   #input you data,if you have test data can transfer to kernel API data()
                                                          #data can be a list,[data1,data2,...,datan]
kernel.train(32,5)         #train neural network
                                                #batch: batch size
                                                #epoch:epoch


-----------------------------------------------------------------------------------------
Parallel optimization:
You can use parallel optimization to speed up your training,parallel optimization speed up training by multiprocessing.
Note have three types of parallel optimization:
1. Perform forward propagation and optimization in parallel. (PO1)

2. Perform forward propagation, one gradient computation or multiple gradient computations and optimization in parallel. (PO2)

3. Perform forward propagation, gradient computation and optimization in parallel without locks. (PO3)


----------------------------------------------------------------------------------------
Save:
This kernel API can save you neural network and param as a file.
kernel.save(path)


----------------------------------------------------------------------------------------
Restore:
If you want to train your neural network again.
import Note.create.DL.kernel as k
kernel=k.kernel()   #don't need neural network object.
kernel.restore(s_path,p_path)   #use this API to restore your neural network and param file.


-----------------------------------------------------------------------------------------
This kernel API can visualize your taining.
visualize_train(self)
kernel.visualize_train()   #use train_visual() by kernel


-----------------------------------------------------------------------------------------
Kernel's instance object:
self.nn: neural network object
self.nn.km: kernel mode
self.PO: parallel optimization
self.thread_lock: thread lock
self.thread: thread sum
self.ol: online training function be used for kernel to oniline train
self.suspend: if you want to suspend training,you can assign self.suspend to True
self.stop: if you want to stop training,you can assign self.stop to True
self.save_epoch: if you want to save your neural network,you can assign self.save_epoch to an epoch
self.batch: batch size
self.epoch: epoch
self.train_loss: train loss
self.train_loss_list: self.train_visual() use this instance to visualize train loss's curve
self.test_flag: test flag if you want test,make self.test_flag=True
self.total_epoch: total epoch
self.time: train time
self.total_time:total train time
self.train_data: train data
self.train_labels: train labels
self.nn.param: neural network object's param list
self.gradient: this instance be used for the second kind of parallel optimization,for reduce memory consumption
self.nn.ec: epoch counter
self.nn.bc: batch counter
self.train_counter: count the number of times the train function has been used


-----------------------------------------------------------------------------------------
Kernel's methon object:
data(self,train_data,train_labels,test_data=None,test_labels=None): assign data for self.train_data,self.train_labels,self.test_data,self.test_labels
init(self,param=None): initialize some kernel's instance
apply_gradient(self,tape,opt,loss,parameter): optimize neural network
loss_acc(self,output=None,labels_batch=None,loss=None,test_batch=None,total_loss=None,total_acc=None,t=None): compute loss and acc
train(self,batch=None,epoch=None,test_batch=None,file_path=None,one=True,p=None,s=None): train neural network
test(self,test_data,test_labels,batch=None,t=None): output test loss and test acc
suspend_func(self): be used to suspend training
train_visual(self): visualize train loss and train acc by matplotlib
save_p(self,path): save neural network's parameter
save(self,path,i=None,one=True): save neural network and neural network's parameter
restore(self,s_path,p_path): restore neural network and neural network's parameter


-----------------------------------------------------------------------------------------
Support suspending training.
Support stop and save in training.
Support stop and save in multithreading training.
Support stop mechanism.
Support gradient attenuation.
Support memory protection mechanism.
Support training priority and memory priority.
