Use kernel to train neural network. First, you should write a neural network class that include method object and Instance object.
Method object:
Method object need to use Tensorflow 2.0 version and above to write.
1. fp(data) forward propagation function,it receive data,return output.
2. loss(output,labels) loss function,it receive output and labels output loss.
3. oopt(gradient,param) and gradient(output,loss,param) If you want to write optimizer by yourself.
You can also do.
1. fp(data,labels) forward propagation function,it receive data and labels,return output and loss.
2. oopt(gradient,param) and gradient(output,loss,param) If you want to write optimizer by yourself.

Instance object:
1.If you don't want to write optimizer by yourself,assign opt as optimizer of tensorflow.
2.Instance include parameter and hyper-parameter,where parameter must be named param, instance can include anything you need.


----------------------------------------------------------------------------------------
If you accomplish your neural network,you can use kernel to train.
example:
import Note.create.DL.kernel as k   #import kernel
import tensorflow as tf                   #import platform
import cnn as c                               #import neural network
mnist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train,x_test =x_train/255.0,x_test/255.0
y_train=tf.one_hot(y_train,10)
cnn=c.cnn()                                #create neural network object
kernel=k.kernel(cnn)                 #start kernel
kernel.platform=tf                           #use platform
kernel.data(x_train,y_train)   #input you data,if you have test data can transfer to kernel API data()
                                                          #data can be a list,[data1,data2,...,datan]
kernel.train(32,5)         #train neural network
                                                #batch: batch size
                                                #epoch:epoch


-----------------------------------------------------------------------------------------
Parallel optimization:
You can use parallel optimization to speed up your training,parallel optimization speed up training by multithreading.
Note have two types of parallel optimization:
1. parallel forward propagation and optimizing.
2. parallel forward propagation and calculate a gradient and optimizing.


Multithreadingï¼š
example:
import Note.create.DL.kernel as k   #import kernel
import tensorflow as tf                   #import platform
import cnn as c                               #import neural network
import threading
mnist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train,x_test =x_train/255.0,x_test/255.0
y_train=tf.one_hot(y_train,10)
cnn=c.cnn()                                #create neural network object
kernel=k.kernel(cnn)   #start kernel
kernel.platform=tf                            #use platform
kernel.thread=2                        #thread count
kernel.PO=1 or kernel.PO=2
kernel.data(x_train,y_train)   #input you data
kernel.thread_lock=[threading.Lock(),threading.Lock()] or kernel.thread_lock=[threading.Lock(),threading.Lock()]
class thread(threading.Thread):
	def run(self):
		kernel.train(32,3)
for _ in range(2):
	_thread=thread()
	_thread.start()
for _ in range(2):
	_thread.join()


Stop multithreading training and saving when condition is met.
example:
import Note.create.DL.kernel as k   #import kernel
import tensorflow as tf                   #import platform
import cnn as c                               #import neural network
import threading
mnist=tf.keras.datasets.mnist
(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train,x_test =x_train/255.0,x_test/255.0
y_train=tf.one_hot(y_train,10)
cnn=c.cnn()                                #create neural network object
kernel=k.kernel(cnn)   #start kernel
kernel.platform=tf                            #use platform
kernel.stop=True
kernel.end_loss=0.7
kernel.thread=2                        #thread count
kernel.PO=1 or kernel.PO=2
kernel.data(x_train,y_train)   #input you data
kernel.thread_lock=[threading.Lock(),threading.Lock()] or kernel.thread_lock=[threading.Lock(),threading.Lock()]
class thread(threading.Thread):
	def run(self):
		kernel.train(32,3)
for _ in range(2):
	_thread=thread()
	_thread.start()
for _ in range(2):
	_thread.join()


----------------------------------------------------------------------------------------
Add threads:
add_threads(self,thread)


----------------------------------------------------------------------------------------
Save:
This kernel API can save you neural network and param as a file.
kernel.save(path)


----------------------------------------------------------------------------------------
Restore:
If you want to train your neural network again.
import Note.create.DL.kernel as k
kernel=k.kernel()   #don't need neural network object.
kernel.restore(s_path,p_path)   #use this API to restore your neural network and param file.


-----------------------------------------------------------------------------------------
This kernel API can set end condition for training.
set_end(self,end_loss=None,end_acc=None,end_test_loss=None,end_test_acc=None)
kernel.set_end(end condition)   #use set_end() by kernel


-----------------------------------------------------------------------------------------
This kernel API can visualize your taining.
visualize_train(self)
kernel.visualize_train()   #use train_visual() by kernel


-----------------------------------------------------------------------------------------
System support suspending training.
System support stop and save in training.
System support stop and save in multithreading training.


-----------------------------------------------------------------------------------------
Kernel's instance object:
self.nn: neural network object
self.nn.km: kernel mode
self.PO: parallel optimization
self.thread_lock: thread lock
self.thread: thread sum
self.ol: online training function be used for kernel to oniline train
self.suspend: if you want to suspend training,you can assign self.suspend to True
self.stop: if you want to stop training,you can assign self.stop to True
self.save_epoch: if you want to save your neural network,you can assign self.save_epoch to an epoch
self.batch: batch size
self.epoch: epoch
self.train_loss: train loss
self.train_loss_list: self.train_visual() use this instance to visualize train loss's curve
self.test_flag: test flag if you want test,make self.test_flag=True
self.total_epoch: total epoch
self.time: train time
self.total_time:total train time
self.train_data: train data
self.train_labels: train labels
self.nn.param: neural network object's param list
self.gradient: this instance be used for the second kind of parallel optimization,for reduce memory consumption
self.nn.ec: epoch counter
self.nn.bc: batch counter
self.train_counter: count the number of times the train function has been used


-----------------------------------------------------------------------------------------
Kernel's methon object:
data(self,train_data,train_labels,test_data=None,test_labels=None): assign data for self.train_data,self.train_labels,self.test_data,self.test_labels
init(self,param=None): initialize some kernel's instance
add_threads(self,thread): add thread count
set_end(self,end_loss=None,end_acc=None,end_test_loss=None,end_test_acc=None): set end condition
apply_gradient(self,tape,opt,loss,parameter): optimize neural network
loss_acc(self,output=None,labels_batch=None,loss=None,test_batch=None,total_loss=None,total_acc=None,t=None): compute loss and acc
core_opt(self,output,loss,t=None): optimize neural network
core_opt_t(self,data,labels,t): optimize neural network by multithreading
train(self,batch=None,epoch=None,test_batch=None,file_path=None,one=True,p=None,s=None): train neural network
test(self,test_data,test_labels,batch=None,t=None): output test loss and test acc
suspend_func(self): be used to suspend training
train_visual(self): visualize train loss and train acc by matplotlib
save_p(self,path): save neural network's parameter
save(self,path,i=None,one=True): save neural network and neural network's parameter
restore(self,s_path,p_path): restore neural network and neural network's parameter
