import tensorflow as tf
import numpy as np
import statistics
import time


class kernel:
    # This is the constructor method for initializing the class attributes
    def __init__(self,nn=None,save_episode=False):
        self.nn=nn # This is the neural network object
        if hasattr(self.nn,'km'): # This is a condition to check if the model has a km attribute
            self.nn.km=1 # This is to assign the value 1 to the km attribute
        self.platform=None # This is the platform for running the algorithm, such as TensorFlow or PyTorch
        self.state_pool=None # This is a pool for storing the states from the environment
        self.action_pool=None # This is a pool for storing the actions taken by the agent
        self.next_state_pool=None # This is a pool for storing the next states from the environment
        self.reward_pool=None # This is a pool for storing the rewards received by the agent
        self.done_pool=None # This is a pool for storing the done flags indicating the end of an episode
        self.episode_set=[] # This is a list for storing the episodes generated by the agent
        self.epsilon=None # This is a parameter for controlling the exploration-exploitation trade-off
        self.episode_step=None # This is a parameter for limiting the number of steps per episode
        self.pool_size=None # This is a parameter for limiting the size of the pools
        self.batch=None # This is a parameter for determining the batch size for training
        self.update_step=None # This is a parameter for determining the frequency of updating the model parameters
        self.trial_count=None # This is a parameter for determining the number of trials for evaluating the performance
        self.criterion=None # This is a parameter for determining the criterion for stopping the training
        self.reward_list=[] # This is a list for storing the rewards per episode
        self.suspend=False # This is a flag for indicating whether to suspend the training or not
        self.save_epi=None # This is a parameter for determining whether to save the episodes or not
        self.max_episode_count=None # This is a parameter for limiting the number of episodes to save
        self.save_episode=save_episode # This is an argument for setting the save_epi parameter
        self.filename='save.dat' # This is a filename for saving or loading the data
        self.loss=None # This is a variable for storing the loss value per batch
        self.loss_list=[] # This is a list for storing the loss values per episode
        self.sc=0 # This is a counter for counting the number of steps taken by the agent
        self.total_episode=0 # This is a counter for counting the number of episodes completed by the agent
        self.time=0 # This is a variable for measuring the time elapsed per episode
        self.total_time=0 # This is a variable for measuring the total time elapsed
    
    # This is a method for creating an action vector based on epsilon-greedy policy
    def action_vec(self):
        if self.epsilon!=None: # This is a condition to check if epsilon is defined or not
            self.action_one=np.ones(self.action_count,dtype=np.int8) # This is to create an array of ones with length equal to action_count, which is the number of possible actions in the environment
            return
    
    
    # This is a method for initializing or resetting some of the class attributes 
    def init(self):
        try:
            if hasattr(self.nn,'pr'): # This is a condition to check if the model has a pr attribute, which stands for prioritized replay buffer 
                self.nn.pr.TD=np.array(0) # This is to assign an array of zero to the TD attribute, which stands for temporal difference error 
        except Exception as e: # This is to catch any exception that may occur during this process 
            raise e # This is to raise or re-throw the exception 
        self.suspend=False # This is to reset the suspend flag to False 
        self.save_epi=None # This is to reset the save_epi parameter to None 
        self.episode_set=[] # This is to reset the episode_set list to an empty list 
        self.state_pool=None # This is to reset the state_pool to None 
        self.action_pool=None # This is to reset the action_pool to None 
        self.next_state_pool=None # This is to reset the next_state_pool to None 
        self.reward_pool=None # This is to reset the reward_pool to None 
        self.done_pool=None # This is to reset the done_pool to None 
        self.reward_list=[] # This is to reset the reward_list to an empty list 
        self.loss=0 # This is to reset the loss value to zero 
        self.loss_list=[] # This is to reset the loss_list to an empty list 
        self.sc=0 # This is to reset the step counter to zero 
        self.total_episode=0 # This is to reset the episode counter to zero 
        self.time=0 # This is to reset the time variable to zero 
        self.total_time=0 # This is to reset the total_time variable to zero
        return
    
    
    # This is a method for setting up some of the class attributes based on the arguments
    def set_up(self,epsilon=None,episode_step=None,pool_size=None,batch=None,update_step=None,trial_count=None,criterion=None):
        if epsilon!=None: # This is a condition to check if epsilon is given or not
            self.epsilon=epsilon # This is to assign the epsilon value to the epsilon attribute
        if episode_step!=None: # This is a condition to check if episode_step is given or not
            self.episode_step=episode_step # This is to assign the episode_step value to the episode_step attribute
        if pool_size!=None: # This is a condition to check if pool_size is given or not
            self.pool_size=pool_size # This is to assign the pool_size value to the pool_size attribute
        if batch!=None: # This is a condition to check if batch is given or not
            self.batch=batch # This is to assign the batch value to the batch attribute
        if update_step!=None: # This is a condition to check if update_step is given or not
            self.update_step=update_step # This is to assign the update_step value to the update_step attribute
        if trial_count!=None: # This is a condition to check if trial_count is given or not
            self.trial_count=trial_count # This is to assign the trial_count value to the trial_count attribute
        if criterion!=None: # This is a condition to check if criterion is given or not
            self.criterion=criterion # This is to assign the criterion value to the criterion attribute
        self.action_vec() # This is to call the action_vec method for creating an action vector based on epsilon-greedy policy
        return
    
    
    # This is a method for implementing an epsilon-greedy policy for choosing an action based on a state
    def epsilon_greedy_policy(self,s):
        action_prob=self.action_one*self.epsilon/len(self.action_one) # This is to create an array of probabilities for each action based on epsilon and action_one, which are class attributes
        try:
            if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                best_a=np.argmax(self.nn.nn.fp(s)) # This is to find the best action based on the neural network model's output for the state s using fp method, which stands for forward propagation
                action_prob[best_a]+=1-self.epsilon # This is to increase the probability of choosing the best action by 1-epsilon
            else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
                s=self.platform.tensor(s,dtype=self.platform.float).to(self.nn.device) # This is to convert the state s into a PyTorch tensor with float data type and send it to the device where the model runs, such as CPU or GPU
                best_a=self.nn.nn(s).argmax() # This is to find the best action based on the neural network model's output for the state s using nn method, which stands for neural network
                action_prob[best_a.numpy()]+=1-self.epsilon  # This is to increase the probability of choosing the best action by 1-epsilon and convert it into a numpy array
        except Exception as e: # This is to catch any exception that may occur during this process 
            raise e # This is to raise or re-throw the exception 
        return action_prob
    
    
    # This is a method for computing the loss and updating the model parameters using TensorFlow
    @tf.function(jit_compile=True)
    def tf_opt(self,state_batch,action_batch,next_state_batch,reward_batch,done_batch):
        with self.platform.GradientTape(persistent=True) as tape: # This is to create a gradient tape object for recording and computing gradients in TensorFlow
            loss=self.nn.loss(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to compute the loss value based on the model's loss function and the data batches
        if hasattr(self.nn,'gradient'): # This is a condition to check if the model has a gradient attribute, which stands for a custom gradient function
            gradient=self.nn.gradient(tape,loss) # This is to compute the gradients based on the model's gradient function and the tape and loss objects
            if hasattr(self.nn.opt,'apply_gradients'): # This is a condition to check if the model's optimizer has an apply_gradients method, which indicates that it is a TensorFlow optimizer
                self.nn.opt.apply_gradients(zip(gradient,self.nn.param)) # This is to apply the gradients to the model's parameters using the optimizer's apply_gradients method
            else: # This means that the model's optimizer does not have an apply_gradients method, which indicates that it is a custom optimizer function
                self.nn.opt(gradient) # This is to update the model's parameters using the optimizer function and the gradients
        else: # This means that the model does not have a gradient attribute, which indicates that it uses the default gradient computation
            if hasattr(self.nn,'nn'): # This is a condition to check if the model has a nn attribute, which stands for a single neural network model
                gradient=tape.gradient(loss,self.nn.param) # This is to compute the gradients based on the tape and loss objects and the model's parameters
                self.nn.opt.apply_gradients(zip(gradient,self.nn.param)) # This is to apply the gradients to the model's parameters using the optimizer's apply_gradients method
            else: # This means that the model does not have a nn attribute, which indicates that it uses two neural network models, such as actor and critic
                actor_gradient=tape.gradient(loss[0],self.nn.param[0]) # This is to compute the gradients for the actor model based on the first element of the loss object and the actor model's parameters
                critic_gradient=tape.gradient(loss[1],self.nn.param[1]) # This is to compute the gradients for the critic model based on the second element of the loss object and the critic model's parameters
                self.nn.opt.apply_gradients(zip(actor_gradient,self.nn.param[0])) # This is to apply the gradients to the actor model's parameters using the optimizer's apply_gradients method
                self.nn.opt.apply_gradients(zip(critic_gradient,self.nn.param[1])) # This is to apply the gradients to the critic model's parameters using the optimizer's apply_gradients method
        return loss # This is to return the loss value
    
    
    # This is a method for computing the loss and updating the model parameters using PyTorch
    def pytorch_opt(self,state_batch,action_batch,next_state_batch,reward_batch,done_batch):
        loss=self.nn.loss(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to compute the loss value based on the model's loss function and the data batches
        self.nn.backward(loss) # This is to compute and accumulate the gradients based on the model's backward method and the loss object
        self.nn.opt() # This is to update and reset the model's parameters based on the model's opt method, which stands for optimizer function
        return loss # This is to return the loss value
    
    
    # This is a method for choosing between tf_opt or pytorch_opt based on the platform attribute 
    def opt(self,state_batch,action_batch,next_state_batch,reward_batch,done_batch):
        if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
            loss=self.tf_opt(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to call the tf_opt method and pass the data batches as arguments
        else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
            loss=self.pytorch_opt(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to call the pytorch_opt method and pass the data batches as arguments
        return loss # This is to return the loss value
    
    
    # This is a method for computing the loss and updating the model parameters using online data
    def opt_ol(self,state,action,next_state,reward,done):
        if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
            loss=self.tf_opt(state,action,next_state,reward,done) # This is to call the tf_opt method and pass the online data as arguments
        else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
            loss=self.pytorch_opt(state,action,next_state,reward,done) # This is to call the pytorch_opt method and pass the online data as arguments
        return loss # This is to return the loss value
    
    
    # This is a method for storing the data into the pools
    def pool(self,s,a,next_s,r,done):
        if type(self.state_pool)!=np.ndarray and self.state_pool==None: # This is a condition to check if the state_pool is not an array and is None, which indicates that it is empty
            self.state_pool=s # This is to assign the state s to the state_pool
            if type(a)==int: # This is a condition to check if the action a is an integer, which indicates that it is a discrete action
                a=np.array(a) # This is to convert the action a into an array
                self.action_pool=np.expand_dims(a,axis=0) # This is to expand the dimension of the action array and assign it to the action_pool
            else: # This means that the action a is not an integer, which indicates that it is a continuous action
                self.action_pool=a # This is to assign the action a to the action_pool
            self.next_state_pool=np.expand_dims(next_s,axis=0) # This is to expand the dimension of the next state next_s and assign it to the next_state_pool
            self.reward_pool=np.expand_dims(r,axis=0) # This is to expand the dimension of the reward r and assign it to the reward_pool
            self.done_pool=np.expand_dims(done,axis=0) # This is to expand the dimension of the done flag done and assign it to the done_pool
        else: # This means that the state_pool is not empty
            self.state_pool=np.concatenate((self.state_pool,s),0) # This is to concatenate or append the state s to the state_pool along axis 0
            if type(a)==int: # This is a condition to check if the action a is an integer, which indicates that it is a discrete action
                a=np.array(a) # This is to convert the action a into an array
                self.action_pool=np.concatenate((self.action_pool,np.expand_dims(a,axis=0)),0) # This is to expand the dimension of the action array and concatenate or append it to the action_pool along axis 0
            else: # This means that the action a is not an integer, which indicates that it is a continuous action
                self.action_pool=np.concatenate((self.action_pool,a),0) # This is to concatenate or append the action a to the action_pool along axis 0
            self.next_state_pool=np.concatenate((self.next_state_pool,np.expand_dims(next_s,axis=0)),0) # This is to expand the dimension of the next state next_s and concatenate or append it to the next_state_pool along axis 0
            self.reward_pool=np.concatenate((self.reward_pool,np.expand_dims(r,axis=0)),0) # This is to expand the dimension of the reward r and concatenate or append it to the reward_pool along axis 0
            self.done_pool=np.concatenate((self.done_pool,np.expand_dims(done,axis=0)),0) # This is to expand the dimension of the done flag done and concatenate or append it to the done_pool along axis 0
        if len(self.state_pool)>self.pool_size: # This is a condition to check if the length of the state_pool exceeds the pool_size attribute, which indicates that it needs trimming 
            self.state_pool=self.state_pool[1:] # This is to remove or pop out the first element of the state_pool 
            self.action_pool=self.action_pool[1:] # This is to remove or pop out the first element of the action_pool 
            self.next_state_pool=self.next_state_pool[1:] # This is to remove or pop out the first element of the next_state_pool 
            self.reward_pool=self.reward_pool[1:] # This is to remove or pop out the first element of the reward_pool 
            self.done_pool=self.done_pool[1:] # This is to remove or pop out the first element of the done pool 
        return
        
    
    # This is a method for training the model using data from pools 
    def _train(self):
        if len(self.state_pool)<self.batch: # This is a condition to check if the length of the state_pool is less than the batch attribute, which indicates that there is not enough data for training
            return np.array(0.) # This is to return a zero value as the loss
        else: # This means that there is enough data for training
            loss=0 # This is to initialize the loss value to zero
            batches=int((len(self.state_pool)-len(self.state_pool)%self.batch)/self.batch) # This is to calculate the number of batches based on the length of the state_pool and the batch attribute
            if len(self.state_pool)%self.batch!=0: # This is a condition to check if there is any remainder after dividing the length of the state_pool by the batch attribute, which indicates that there is an incomplete batch
                batches+=1 # This is to increase the number of batches by one to include the incomplete batch
            if hasattr(self.nn,'data_func'): # This is a condition to check if the model has a data_func attribute, which stands for a custom data processing function
                for j in range(batches): # This is a loop for iterating over each batch
                    self.suspend_func() # This is to call the suspend_func method for suspending the training if needed
                    state_batch,action_batch,next_state_batch,reward_batch,done_batch=self.nn.data_func(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,self.batch) # This is to get a batch of data from the pools using the model's data_func method
                    batch_loss=self.opt(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to call the opt method for computing the loss and updating the model parameters using the batch of data
                    loss+=batch_loss # This is to accumulate the loss value over each batch
                    if hasattr(self.nn,'bc'): # This is a condition to check if the model has a bc attribute, which stands for batch counter
                        try:
                            self.nn.bc.assign_add(1) # This is to increase the bc value by one using assign_add method, which works for TensorFlow variables
                        except Exception: # This is to catch any exception that may occur during this process 
                            self.nn.bc+=1 # This is to increase the bc value by one using normal addition, which works for PyTorch variables
                if len(self.state_pool)%self.batch!=0: # This is a condition to check if there is any remainder after dividing the length of the state_pool by the batch attribute, which indicates that there is an incomplete batch
                    self.suspend_func() # This is to call the suspend_func method for suspending the training if needed
                    state_batch,action_batch,next_state_batch,reward_batch,done_batch=self.nn.data_func(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,self.batch) # This is to get a batch of data from the pools using the model's data_func method
                    batch_loss=self.opt(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to call the opt method for computing the loss and updating the model parameters using the batch of data
                    loss+=batch_loss # This is to accumulate the loss value over each batch
                    if hasattr(self.nn,'bc'): # This is a condition to check if the model has a bc attribute, which stands for batch counter
                        try:
                            self.nn.bc.assign_add(1) # This is to increase the bc value by one using assign_add method, which works for TensorFlow variables
                        except Exception: # This is to catch any exception that may occur during this process 
                            self.nn.bc+=1 # This is to increase the bc value by one using normal addition, which works for PyTorch variables
            else: # This means that the model does not have a data_func attribute, which indicates that it uses the default data processing method
                j=0 # This is to initialize a counter for counting batches 
                train_ds=tf.data.Dataset.from_tensor_slices((self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool)).shuffle(len(self.state_pool)).batch(self.batch) # This is to create a TensorFlow dataset object from the pools and shuffle and batch them 
                for state_batch,action_batch,next_state_batch,reward_batch,done_batch in train_ds: # This is a loop for iterating over each batch from the dataset object 
                    self.suspend_func() # This is to call the suspend_func method for suspending the training if needed
                    if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                        pass # This means that there is no need to convert or modify the data batches 
                    else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch 
                        state_batch=state_batch.numpy() # This is to convert the state batch into a numpy array
                        action_batch=action_batch.numpy() # This is to convert the action batch into a numpy array
                        next_state_batch=next_state_batch.numpy() # This is to convert the next state batch into a numpy array
                        reward_batch=reward_batch.numpy() # This is to convert the reward batch into a numpy array
                        done_batch=done_batch.numpy() # This is to convert the done batch into a numpy array
                    batch_loss=self.opt(state_batch,action_batch,next_state_batch,reward_batch,done_batch) # This is to call the opt method for computing the loss and updating the model parameters using the batch of data
                    loss+=batch_loss # This is to accumulate the loss value over each batch
                    j+=1 # This is to increase the counter by one 
                    if hasattr(self.nn,'bc'): # This is a condition to check if the model has a bc attribute, which stands for batch counter
                        try:
                            self.nn.bc.assign_add(1) # This is to increase the bc value by one using assign_add method, which works for TensorFlow variables
                        except Exception: # This is to catch any exception that may occur during this process 
                            self.nn.bc+=1 # This is to increase the bc value by one using normal addition, which works for PyTorch variables
            if self.update_step!=None: # This is a condition to check if the update_step attribute is defined or not, which indicates whether to update the model parameters periodically or after each episode
                if self.sc%self.update_step==0: # This is a condition to check if the step counter is divisible by the update_step attribute, which indicates that it is time to update the model parameters
                    self.nn.update_param() # This is to call the update_param method of the model for updating its parameters
            else: # This means that the update_step attribute is not defined, which indicates that the model parameters are updated after each episode
                self.nn.update_param() # This is to call the update_param method of the model for updating its parameters
        if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
            loss=loss.numpy()/batches # This is to convert the loss value into a numpy array and divide it by the number of batches to get the average loss per batch
        else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch 
            loss=loss.detach().numpy()/batches # This is to detach the loss value from the computation graph and convert it into a numpy array and divide it by the number of batches to get the average loss per batch
        return loss # This is to return the loss value
    
    
    # This is a method for training the model using data from an episode 
    def train_(self):
        episode=[] # This is to initialize an empty list for storing an episode 
        self.reward=0 # This is to initialize the reward value to zero 
        s=self.nn.env(initial=True) # This is to get an initial state from the environment using the model's env method, which stands for environment function 
        if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
            if type(self.nn.param[0])!=list: # This is a condition to check if the first element of the model's param attribute, which stands for parameters, is not a list, which indicates that it has only one neural network model 
                s=np.array(s,self.nn.param[0].dtype.name) # This is to convert the state s into an array with data type matching with the model's parameter data type 
            else: # This means that the first element of the model's param attribute is a list, which indicates that it has two neural network models, such as actor and critic
                s=np.array(s,self.nn.param[0][0].dtype.name) # This is to convert the state s into an array with data type matching with the first model's parameter data type 
        else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
            s=self.platform.tensor(s,dtype=self.platform.float).to(self.nn.device) # This is to convert the state s into a PyTorch tensor with float data type and send it to the device where the model runs, such as CPU or GPU
        if self.episode_step==None: # This is a condition to check if the episode_step attribute is None, which indicates that there is no limit on the number of steps per episode
            while True: # This is an infinite loop for running an episode until it ends
                self.suspend_func() # This is to call the suspend_func method for suspending the training if needed
                if hasattr(self.nn,'nn'): # This is a condition to check if the model has a nn attribute, which stands for a single neural network model
                    if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        action_prob=self.epsilon_greedy_policy(s) # This is to call the epsilon_greedy_policy method for choosing an action based on a state and return its probability
                        a=np.random.choice(self.action_count,p=action_prob) # This is to sample an action from a discrete distribution with probabilities given by action_prob
                    else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        action_prob=self.epsilon_greedy_policy(s) # This is to call the epsilon_greedy_policy method for choosing an action based on a state and return its probability
                        a=np.random.choice(self.action_count,p=action_prob) # This is to sample an action from a discrete distribution with probabilities given by action_prob
                else: # This means that the model does not have a nn attribute, which indicates that it uses two neural network models, such as actor and critic
                    if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                            a=self.nn.action(s) # This is to get an action from the model's action method, which stands for action function, based on the state s
                            reward=self.nn.discriminator(s,a) # This is to get a reward from the model's discriminator method, which stands for discriminator function, based on the state s and action a
                            s=np.squeeze(s) # This is to remove any singleton dimensions from the state s
                        else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                            a=self.nn.action(s).numpy() # This is to get an action from the model's action method, which stands for action function, based on the state s and convert it into a numpy array
                    else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                            a=self.nn.action(s) # This is to get an action from the model's action method, which stands for action function, based on the state s
                            reward=self.nn.discriminator(s,a) # This is to get a reward from the model's discriminator method, which stands for discriminator function, based on the state s and action a
                            s=np.squeeze(s) # This is to remove any singleton dimensions from the state s
                        else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                            a=self.nn.action(s).detach().numpy() # This is to get an action from the model's action method, which stands for action function, based on the state s and detach it from the computation graph and convert it into a numpy array
                next_s,r,done=self.nn.env(a) # This is to get the next state next_s, reward r and done flag done from the environment using the model's env method based on the action a 
                if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                    if type(self.nn.param[0])!=list: # This is a condition to check if the first element of the model's param attribute, which stands for parameters, is not a list, which indicates that it has only one neural network model 
                        next_s=np.array(next_s,self.nn.param[0].dtype.name) # This is to convert the next state next_s into an array with data type matching with the model's parameter data type 
                        r=np.array(r,self.nn.param[0].dtype.name) # This is to convert the reward r into an array with data type matching with the model's parameter data type 
                        done=np.array(done,self.nn.param[0].dtype.name) # This is to convert the done flag done into an array with data type matching with the model's parameter data type 
                    else: # This means that the first element of the model's param attribute is a list, which indicates that it has two neural network models, such as actor and critic
                        next_s=np.array(next_s,self.nn.param[0][0].dtype.name) # This is to convert the next state next_s into an array with data type matching with the first model’s parameter data type
                        r=np.array(r,self.nn.param[0][0].dtype.name) # This is to convert the reward r into an array with data type matching with the first model's parameter data type 
                    done=np.array(done,self.nn.param[0][0].dtype.name) # This is to convert the done flag done into an array with data type matching with the first model's parameter data type 
            if hasattr(self.nn,'pool'): # This is a condition to check if the model has a pool attribute, which stands for a custom pool function
                if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                    self.nn.pool(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,[s,a,next_s,reward,done]) # This is to call the model's pool method for storing the data into the pools using the state s, action a, next state next_s, reward reward and done flag done
                else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                    self.nn.pool(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,[s,a,next_s,r,done]) # This is to call the model's pool method for storing the data into the pools using the state s, action a, next state next_s, reward r and done flag done
            else: # This means that the model does not have a pool attribute, which indicates that it uses the default pool method
                self.pool(s,a,next_s,r,done) # This is to call the self.pool method for storing the data into the pools using the state s, action a, next state next_s, reward r and done flag done
            if hasattr(self.nn,'pr'): # This is a condition to check if the model has a pr attribute, which stands for priority replay
                self.nn.pr.TD=np.append(self.nn.pr.TD,self.nn.initial_TD) # This is to append the initial TD value returned by the model's initial_TD method, which stands for initial temporal difference function, to the TD attribute of pr, which stands for temporal difference values
                if len(self.state_pool)>self.pool_size: # This is a condition to check if the length of the state_pool exceeds the pool_size attribute, which indicates that it needs trimming 
                    TD=np.array(0) # This is to create an array with zero value as TD
                    self.nn.pr.TD=np.append(TD,self.nn.pr.TD[2:]) # This is to append TD to the TD attribute of pr after removing or popping out the first two elements
            self.reward=r+self.reward # This is to update the reward value by adding r to it
            loss=self._train() # This is to call the _train method for training the model using data from pools and return the loss value
            self.sc+=1 # This is to increase the step counter by one 
            if done: # This is a condition to check if the done flag is True, which indicates that the episode has ended
                if self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                    episode=[s,a,next_s,r] # This is to assign a list of state s, action a, next state next_s and reward r as an episode 
                self.reward_list.append(self.reward) # This is to append the reward value to the reward_list attribute, which stands for a list of rewards per episode 
                return loss,episode,done # This is to return the loss value, episode and done flag as outputs
            elif self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                episode=[s,a,next_s,r] # This is to assign a list of state s, action a, next state next_s and reward r as an episode 
            s=next_s # This is to update the state s by assigning next_s to it
        else: # This is an else clause for the case when the episode_step attribute is not None, which indicates that there is a limit on the number of steps per episode
            for _ in range(self.episode_step): # This is a loop for running an episode for a fixed number of steps
                self.suspend_func() # This is to call the suspend_func method for suspending the training if needed
                if hasattr(self.nn,'nn'): # This is a condition to check if the model has a nn attribute, which stands for a single neural network model
                    if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        action_prob=self.epsilon_greedy_policy(s) # This is to call the epsilon_greedy_policy method for choosing an action based on a state and return its probability
                        a=np.random.choice(self.action_count,p=action_prob) # This is to sample an action from a discrete distribution with probabilities given by action_prob
                    else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        action_prob=self.epsilon_greedy_policy(s) # This is to call the epsilon_greedy_policy method for choosing an action based on a state and return its probability
                        a=np.random.choice(self.action_count,p=action_prob) # This is to sample an action from a discrete distribution with probabilities given by action_prob
                else: # This means that the model does not have a nn attribute, which indicates that it uses two neural network models, such as actor and critic
                    if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                            a=self.nn.action(s) # This is to get an action from the model's action method, which stands for action function, based on the state s
                            reward=self.nn.discriminator(s,a) # This is to get a reward from the model's discriminator method, which stands for discriminator function, based on the state s and action a
                            s=np.squeeze(s) # This is to remove any singleton dimensions from the state s
                        else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                            a=self.nn.action(s).numpy() # This is to get an action from the model's action method, which stands for action function, based on the state s and convert it into a numpy array
                    else: # This means that the platform does not have a DType attribute, which indicates that it is PyTorch
                        s=np.expand_dims(s,axis=0) # This is to expand the dimension of the state s along axis 0
                        if self.epsilon==None: # This is a condition to check if the epsilon attribute is None, which indicates that it needs to be initialized
                            self.epsilon=self.nn.epsilon(self.sc) # This is to assign the epsilon value returned by the model's epsilon method, which stands for epsilon function, to the epsilon attribute
                        if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                            a=self.nn.action(s) # This is to get an action from the model's action method, which stands for action function, based on the state s
                            reward=self.nn.discriminator(s,a) # This is to get a reward from the model's discriminator method, which stands for discriminator function, based on the state s and action a
                            s=np.squeeze(s) # This is to remove any singleton dimensions from the state s
                        else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                            a=self.nn.action(s).detach().numpy() # This is to get an action from the model's action method, which stands for action function, based on the state s and detach it from the computation graph and convert it into a numpy array
                next_s,r,done=self.nn.env(a) # This is to get the next state next_s, reward r and done flag done from the environment using the model's env method based on the action a 
                if hasattr(self.platform,'DType'): # This is a condition to check if the platform has a DType attribute, which indicates that it is TensorFlow
                    if type(self.nn.param[0])!=list: # This is a condition to check if the first element of the model's param attribute, which stands for parameters, is not a list, which indicates that it has only one neural network model 
                        next_s=np.array(next_s,self.nn.param[0].dtype.name) # This is to convert the next state next_s into an array with data type matching with the model's parameter data type 
                        r=np.array(r,self.nn.param[0].dtype.name) # This is to convert the reward r into an array with data type matching with the model's parameter data type 
                        done=np.array(done,self.nn.param[0].dtype.name) # This is to convert the done flag done into an array with data type matching with the model's parameter data type 
                    else: # This means that the first element of the model's param attribute is a list, which indicates that it has two neural network models, such as actor and critic
                        next_s=np.array(next_s,self.nn.param[0][0].dtype.name) # This is to convert the next state next_s into an array with data type matching with the first model’s parameter data type
                        r=np.array(r,self.nn.param[0][0].dtype.name) # This is to convert the reward r into an array with data type matching with the first model's parameter data type 
                    done=np.array(done,self.nn.param[0][0].dtype.name) # This is to convert the done flag done into an array with data type matching with the first model's parameter data type 
            if hasattr(self.nn,'pool'): # This is a condition to check if the model has a pool attribute, which stands for a custom pool function
                if hasattr(self.nn,'discriminator'): # This is a condition to check if the model has a discriminator attribute, which stands for a discriminator model that evaluates the quality of an action
                    self.nn.pool(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,[s,a,next_s,reward,done]) # This is to call the model's pool method for storing the data into the pools using the state s, action a, next state next_s, reward reward and done flag done
                else: # This means that the model does not have a discriminator attribute, which indicates that it uses the environment's reward function
                    self.nn.pool(self.state_pool,self.action_pool,self.next_state_pool,self.reward_pool,self.done_pool,[s,a,next_s,r,done]) # This is to call the model's pool method for storing the data into the pools using the state s, action a, next state next_s, reward r and done flag done
            else: # This means that the model does not have a pool attribute, which indicates that it uses the default pool method
                self.pool(s,a,next_s,r,done) # This is to call the self.pool method for storing the data into the pools using the state s, action a, next state next_s, reward r and done flag done
            if hasattr(self.nn,'pr'): # This is a condition to check if the model has a pr attribute, which stands for priority replay
                self.nn.pr.TD=np.append(self.nn.pr.TD,self.nn.initial_TD) # This is to append the initial TD value returned by the model's initial_TD method, which stands for initial temporal difference function, to the TD attribute of pr, which stands for temporal difference values
                if len(self.state_pool)>self.pool_size: # This is a condition to check if the length of the state_pool exceeds the pool_size attribute, which indicates that it needs trimming 
                    TD=np.array(0) # This is to create an array with zero value as TD
                    self.nn.pr.TD=np.append(TD,self.nn.pr.TD[2:]) # This is to append TD to the TD attribute of pr after removing or popping out the first two elements
            self.reward=r+self.reward # This is to update the reward value by adding r to it
            loss=self._train() # This is to call the _train method for training the model using data from pools and return the loss value
            self.sc+=1 # This is to increase the step counter by one 
            if done: # This is a condition to check if the done flag is True, which indicates that the episode has ended
                if self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                    episode=[s,a,next_s,r] # This is to assign a list of state s, action a, next state next_s and reward r as an episode 
                self.reward_list.append(self.reward) # This is to append the reward value to the reward_list attribute, which stands for a list of rewards per episode 
                return loss,episode,done # This is to return the loss value, episode and done flag as outputs
            elif self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                episode=[s,a,next_s,r] # This is to assign a list of state s, action a, next state next_s and reward r as an episode 
            s=next_s # This is to update the state s by assigning next_s to it
        self.reward_list.append(self.reward) # This is to append the reward value to the reward_list attribute, which stands for a list of rewards per episode
        return loss,episode,done # This is to return the loss value, episode and done flag as outputs

    
    # This is a function for training the model using data from episodes or pools
    def train(self,episode_count,save=None,one=True,p=None,s=None):
        avg_reward=None # This is to initialize the average reward value to None
        if p==None: # This is a condition to check if the p parameter is None, which indicates that it needs to be assigned a default value
            self.p=9 # This is to assign 9 to the p attribute, which stands for the frequency of printing the loss and reward values
        else: # This means that the p parameter is not None, which indicates that it has a given value
            self.p=p-1 # This is to assign p-1 to the p attribute, which stands for the frequency of printing the loss and reward values
        if s==None: # This is a condition to check if the s parameter is None, which indicates that it needs to be assigned a default value
            self.s=1 # This is to assign 1 to the s attribute, which stands for the frequency of saving the model parameters
            self.file_list=None # This is to assign None to the file_list attribute, which stands for a list of file names for saving the model parameters
        else: # This means that the s parameter is not None, which indicates that it has a given value
            self.s=s-1 # This is to assign s-1 to the s attribute, which stands for the frequency of saving the model parameters
            self.file_list=[] # This is to initialize an empty list for the file_list attribute, which stands for a list of file names for saving the model parameters
        if episode_count!=None: # This is a condition to check if the episode_count parameter is not None, which indicates that it has a given value
            for i in range(episode_count): # This is a loop for running a given number of episodes
                t1=time.time() # This is to record the start time of an episode
                loss,episode,done=self.train_() # This is to call the train_ method for training the model using data from an episode and return the loss value, episode and done flag
                if self.trial_count!=None: # This is a condition to check if the trial_count attribute is not None, which indicates that it has a defined value
                    if len(self.reward_list)>=self.trial_count: # This is a condition to check if the length of the reward_list attribute, which stands for a list of rewards per episode, is greater than or equal to the trial_count attribute, which stands for the number of episodes for calculating the average reward
                        avg_reward=statistics.mean(self.reward_list[-self.trial_count:]) # This is to calculate the average reward value by taking the mean of the last trial_count elements of the reward_list attribute
                        if self.criterion!=None and avg_reward>=self.criterion: # This is a condition to check if the criterion attribute is not None and the average reward value is greater than or equal to the criterion attribute, which indicates that the model has reached a desired performance level
                            t2=time.time() # This is to record the end time of an episode
                            self.total_time+=(t2-t1) # This is to update the total_time attribute, which stands for the total time spent on training, by adding (t2-t1), which stands for the time spent on an episode
                            time_=self.total_time-int(self.total_time) # This is to calculate the decimal part of the total_time attribute
                            if time_<0.5: # This is a condition to check if the decimal part of the total_time attribute is less than 0.5, which indicates that it needs rounding down 
                                self.total_time=int(self.total_time) # This is to round down the total_time attribute by converting it into an integer
                            else: # This means that the decimal part of the total_time attribute is greater than or equal to 0.5, which indicates that it needs rounding up 
                                self.total_time=int(self.total_time)+1 # This is to round up
                            print('episode:{0}'.format(self.total_episode)) # This is to print the episode number
                            print('last loss:{0:.6f}'.format(loss)) # This is to print the last loss value with six decimal places
                            print('average reward:{0}'.format(avg_reward)) # This is to print the average reward value 
                            print() # This is to print an empty line for spacing 
                            print('time:{0}s'.format(self.total_time)) # This is to print the total time spent on training in seconds
                            return # This is to return from the function
                self.loss=loss # This is to assign the loss value to the loss attribute
                self.loss_list.append(loss) # This is to append the loss value to the loss_list attribute, which stands for a list of losses per episode
                self.total_episode+=1 # This is to increase the total_episode attribute, which stands for the total number of episodes, by one
                if episode_count%10!=0: # This is a condition to check if the episode_count parameter is not divisible by 10, which indicates that it needs some adjustment for calculating p and s
                    p=episode_count-episode_count%self.p # This is to subtract the remainder of dividing the episode_count parameter by the p attribute from the episode_count parameter
                    p=int(p/self.p) # This is to divide p by the p attribute and convert it into an integer
                    s=episode_count-episode_count%self.s # This is to subtract the remainder of dividing the episode_count parameter by the s attribute from the episode_count parameter
                    s=int(s/self.s) # This is to divide s by the s attribute and convert it into an integer
                else: # This means that the episode_count parameter is divisible by 10, which indicates that it does not need any adjustment for calculating p and s
                    p=episode_count/(self.p+1) # This is to divide the episode_count parameter by (the p attribute plus one)
                    p=int(p) # This is to convert p into an integer
                    s=episode_count/(self.s+1) # This is to divide the episode_count parameter by (the s attribute plus one)
                    s=int(s) # This is to convert s into an integer
                if p==0: # This is a condition to check if p is zero, which indicates that it needs to be assigned a minimum value of one
                    p=1 # This is to assign one to p
                if s==0: # This is a condition to check if s is zero, which indicates that it needs to be assigned a minimum value of one
                    s=1 # This is to assign one to s
                if i%p==0: # This is a condition to check if i, which stands for the episode index, is divisible by p, which indicates that it is time to print the loss and reward values
                    if len(self.state_pool)>=self.batch: # This is a condition to check if the length of the state_pool attribute, which stands for a pool of states, is greater than or equal to the batch attribute, which stands for a batch size for training
                        print('episode:{0}   loss:{1:.6f}'.format(i+1,loss)) # This is to print the episode number and the loss value with six decimal places
                    if avg_reward!=None: # This is a condition to check if the average reward value is not None, which indicates that it has been calculated
                        print('episode:{0}   average reward:{1}'.format(i+1,avg_reward)) # This is to print the episode number and the average reward value 
                    else: # This means that the average reward value is None, which indicates that it has not been calculated yet 
                        print('episode:{0}   reward:{1}'.format(i+1,self.reward)) # This is to print the episode number and the reward value 
                    print() # This is to print an empty line for spacing 
                if save!=None and i%s==0: # This is a condition to check if the save parameter is not None and i, which stands for the episode index, is divisible by s, which indicates that it is time to save the model parameters
                    self.save(self.total_episode,one) # This is to call the save method for saving the model parameters using the total_episode attribute as an identifier and one as a flag for indicating whether to overwrite or append files 
                if self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                    if done: # This is a condition to check if done flag
                        episode.append('done')
                    self.episode_set.append(episode) # This is to append the episode list to the episode_set attribute, which stands for a set of episodes
                    if self.max_episode_count!=None and len(self.episode_set)>=self.max_episode_count: # This is a condition to check if the max_episode_count attribute is not None and the length of the episode_set attribute is greater than or equal to the max_episode_count attribute, which indicates that it has reached the maximum number of episodes to save
                        self.save_episode=False # This is to assign False to the save_episode attribute, which indicates that it does not need to save any more episodes
                try: # This is a try block for handling any possible errors
                    try: # This is a nested try block for handling any possible errors
                        self.nn.ec.assign_add(1) # This is to call the assign_add method of the ec attribute of the model, which stands for episode counter, to increase it by one
                    except Exception: # This is an except block for catching any exceptions raised by the nested try block
                        self.nn.ec+=1 # This is to increase the ec attribute of the model by one using the += operator
                except Exception: # This is an except block for catching any exceptions raised by the outer try block
                    pass # This is a pass statement for doing nothing and continuing the execution
                t2=time.time() # This is to record the end time of an episode
                self.time+=(t2-t1) # This is to update the time attribute, which stands for the time spent on an episode, by adding (t2-t1), which stands for the time difference between t2 and t1
        else: # This is an else clause for the condition of the episode_count parameter, which indicates that it does not have a given value
            i=0 # This is to initialize the episode index to zero
            while True: # This is an infinite loop for running episodes until the model reaches the desired performance level
                t1=time.time() # This is to record the start time of an episode
                loss,episode,done=self.train_() # This is to call the train_ method for training the model using data from an episode and return the loss value, episode and done flag
                if self.trial_count!=None: # This is a condition to check if the trial_count attribute is not None, which indicates that it has a defined value
                    if len(self.reward_list)==self.trial_count: # This is a condition to check if the length of the reward_list attribute, which stands for a list of rewards per episode, is equal to the trial_count attribute, which stands for the number of episodes for calculating the average reward
                        avg_reward=statistics.mean(self.reward_list[-self.trial_count:]) # This is to calculate the average reward value by taking the mean of the last trial_count elements of the reward_list attribute
                        if self.criterion!=None and avg_reward>=self.criterion: # This is a condition to check if the criterion attribute is not None and the average reward value is greater than or equal to the criterion attribute, which indicates that the model has reached a desired performance level
                            t2=time.time() # This is to record the end time of an episode
                            self.total_time+=(t2-t1) # This is to update the total_time attribute, which stands for the total time spent on training, by adding (t2-t1), which stands for the time spent on an episode
                            time_=self.total_time-int(self.total_time) # This is to calculate the decimal part of the total_time attribute
                            if time_<0.5: # This is a condition to check if the decimal part of the total_time attribute is less than 0.5, which indicates that it needs rounding down 
                                self.total_time=int(self.total_time) # This is to round down the total_time attribute by converting it into an integer
                            else: # This means that the decimal part of the total_time attribute is greater than or equal to 0.5, which indicates that it needs rounding up 
                                self.total_time=int(self.total_time)+1 # This is to round up
                            print('episode:{0}'.format(self.total_episode)) # This is to print the episode number
                            print('last loss:{0:.6f}'.format(loss)) # This is to print the last loss value with six decimal places
                            print('average reward:{0}'.format(avg_reward)) # This is to print the average reward value 
                            print() # This is to print an empty line for spacing 
                            print('time:{0}s'.format(self.total_time)) # This is to print the total time spent on training in seconds
                            return # This is to return from the function
                self.loss=loss # This is to assign the loss value to the loss attribute
                self.loss_list.append(loss) # This is to append the loss value to the loss_list attribute, which stands for a list of losses per episode
                self.total_episode+=1 # This is to increase the total_episode attribute, which stands for the total number of episodes, by one
                if episode_count%10!=0: # This is a condition to check if the episode_count parameter is not divisible by 10, which indicates that it needs some adjustment for calculating p and s
                    p=episode_count-episode_count%self.p # This is to subtract the remainder of dividing the episode_count parameter by the p attribute from the episode_count parameter
                    p=int(p/self.p) # This is to divide p by the p attribute and convert it into an integer
                    s=episode_count-episode_count%self.s # This is to subtract the remainder of dividing the episode_count parameter by the s attribute from the episode_count parameter
                    s=int(s/self.s) # This is to divide s by the s attribute and convert it into an integer
                else: # This means that the episode_count parameter is divisible by 10, which indicates that it does not need any adjustment for calculating p and s
                    p=episode_count/(self.p+1) # This is to divide the episode_count parameter by (the p attribute plus one)
                    p=int(p) # This is to convert p into an integer
                    s=episode_count/(self.s+1) # This is to divide the episode_count parameter by (the s attribute plus one)
                    s=int(s) # This is to convert s into an integer
                if p==0: # This is a condition to check if p is zero, which indicates that it needs to be assigned a minimum value of one
                    p=1 # This is to assign one to p
                if s==0: # This is a condition to check if s is zero, which indicates that it needs to be assigned a minimum value of one
                    s=1 # This is to assign one to s
                if i%p==0: # This is a condition to check if i, which stands for the episode index, is divisible by p, which indicates that it is time to print the loss and reward values
                    if len(self.state_pool)>=self.batch: # This is a condition to check if the length of the state_pool attribute, which stands for a pool of states, is greater than or equal to the batch attribute, which stands for a batch size for training
                        print('episode:{0}   loss:{1:.6f}'.format(i+1,loss)) # This is to print the episode number and the loss value with six decimal places
                    if avg_reward!=None: # This is a condition to check if the average reward value is not None, which indicates that it has been calculated
                        print('episode:{0}   average reward:{1}'.format(i+1,avg_reward)) # This is to print the episode number and the average reward value 
                    else: # This means that the average reward value is None, which indicates that it has not been calculated yet 
                        print('episode:{0}   reward:{1}'.format(i+1,self.reward)) # This is to print the episode number and the reward value 
                    print() # This is to print an empty line for spacing 
                if save!=None and i%s==0: # This is a condition to check if the save parameter is not None and i, which stands for the episode index, is divisible by s, which indicates that it is time to save the model parameters
                    self.save(self.total_episode,one) # This is to call the save method for saving the model parameters using the total_episode attribute as an identifier and one as a flag for indicating whether to overwrite or append files 
                if self.save_episode==True: # This is a condition to check if the save_episode attribute is True, which indicates that it needs to save an episode 
                    if done: # This is a condition to check if done flag
                        episode.append('done')
                    self.episode_set.append(episode) # This is to append the episode list to the episode_set attribute, which stands for a set of episodes
                    if self.max_episode_count!=None and len(self.episode_set)>=self.max_episode_count: # This is a condition to check if the max_episode_count attribute is not None and the length of the episode_set attribute is greater than or equal to the max_episode_count attribute, which indicates that it has reached the maximum number of episodes to save
                        self.save_episode=False # This is to assign False to the save_episode attribute, which indicates that it does not need to save any more episodes
                try: # This is a try block for handling any possible errors
                    try: # This is a nested try block for handling any possible errors
                        self.nn.ec.assign_add(1) # This is to call the assign_add method of the ec attribute of the model, which stands for episode counter, to increase it by one
                    except Exception: # This is an except block for catching any exceptions raised by the nested try block
                        self.nn.ec+=1 # This is to increase the ec attribute of the model by one using the += operator
                except Exception: # This is an except block for catching any exceptions raised by the outer try block
                    pass # This is a pass statement for doing nothing and continuing the execution
                t2=time.time() # This is to record the end time of an episode
                self.time+=(t2-t1) # This is to update the time attribute, which stands for the time spent on an episode, by adding (t2-t1), which stands for the time difference between t2 and t1
                time_=self.time-int(self.time) 
        time_=self.time-int(self.time) # This is to calculate the decimal part of the time attribute, which stands for the time spent on an episode
        if time_<0.5: # This is a condition to check if the decimal part of the time attribute is less than 0.5, which indicates that it needs rounding down 
            self.total_time=int(self.time) # This is to round down the time attribute by converting it into an integer and assign it to the total_time attribute, which stands for the total time spent on training
        else: # This means that the decimal part of the time attribute is greater than or equal to 0.5, which indicates that it needs rounding up 
            self.total_time=int(self.time)+1 # This is to round up the time attribute by converting it into an integer and adding one and assign it to the total_time attribute
        self.total_time+=self.time # This is to update the total_time attribute by adding the time attribute to it
        print('last loss:{0:.6f}'.format(loss)) # This is to print the last loss value with six decimal places
        print('last reward:{0}'.format(self.reward)) # This is to print the last reward value
        print() # This is to print an empty line for spacing 
        print('time:{0}s'.format(self.time)) # This is to print the time spent on an episode in seconds
        return

    
    # This is a function for training the model online using data from the environment
    def train_online(self):
        while True: # This is an infinite loop for running online training until the model stops or suspends
            if hasattr(self.nn,'save'): # This is a condition to check if the model has a save attribute, which stands for a save function
                self.nn.save(self.save) # This is to call the model's save method for saving the model parameters using the save attribute, which stands for a file name
            if hasattr(self.nn,'stop_flag'): # This is a condition to check if the model has a stop_flag attribute, which stands for a flag for stopping the training
                if self.nn.stop_flag==True: # This is a condition to check if the stop_flag attribute is True, which indicates that the training needs to stop
                    return # This is to return from the function
            if hasattr(self.nn,'stop_func'): # This is a condition to check if the model has a stop_func attribute, which stands for a stop function
                if self.nn.stop_func(): # This is to call the model's stop_func method and check if it returns True, which indicates that the training needs to stop
                    return # This is to return from the function
            if hasattr(self.nn,'suspend_func'): # This is a condition to check if the model has a suspend_func attribute, which stands for a suspend function
                self.nn.suspend_func() # This is to call the model's suspend_func method for suspending the training if needed
            data=self.nn.online() # This is to call the model's online method for getting data from the environment and assign it to data
            if data=='stop': # This is a condition to check if data is 'stop', which indicates that the training needs to stop
                return # This is to return from the function
            elif data=='suspend': # This is a condition to check if data is 'suspend', which indicates that the training needs to suspend
                self.nn.suspend_func() # This is to call the model's suspend_func method for suspending the training
            loss=self.opt_ol(data[0],data[1],data[2],data[3],data[4]) # This is to call the opt_ol method for optimizing the model using data and return the loss value. The data consists of five elements: state, action, next state, reward and done flag.
            loss=loss.numpy() # This is to convert the loss value into a numpy array
            self.nn.train_loss_list.append(loss) # This is to append the loss value to the train_loss_list attribute of the model, which stands for a list of losses per step
            if len(self.nn.train_acc_list)==self.nn.max_length: # This is a condition to check if the length of the train_acc_list attribute of the model, which stands for a list of accuracies per step, is equal to the max_length attribute of the model, which stands for the maximum length of lists
                del self.nn.train_acc_list[0] # This is to delete or pop out the first element of the train_acc_list attribute of the model, which stands for removing the oldest accuracy value
            if hasattr(self.nn,'counter'): # This is a condition to check if the model has a counter attribute, which stands for a counter for steps or epochs
                self.nn.counter+=1 # This is to increase the counter attribute
        return